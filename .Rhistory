summary(getTaskData(num_bed_task_m))
ggplot(houses, aes(is.factor(is_waterfront, price)))+
geom_boxplot()
ggplot(houses, aes(is.factor(is_waterfront), price))+
geom_boxplot()
ggplot(houses, aes(is_waterfront, price))+
geom_boxplot()
summary(as.factor(houses$is_waterfront)
summary(as.factor(houses$is_waterfront))
summary(as.factor(houses$is_waterfront))
ggplot(houses, aes(factor(is_waterfront), price))+
geom_boxplot()
categoricalCoerc <- houses %>%
select(c(num_bed, num_bath, num_floors, property_d, condition, renovation_date)) %>%
mutate(num_bed_c = ifelse(num_bed < 3,
ifelse(num_bed > 4, 5, 2), num_bed),
num_bath_c = ifelse(num_bath < 1.5,
ifelse(num_bath > 3.5, 3.75, 1.25), num_bath),
num_floors_c = ifelse(num_floors > 2, 2.5, num_floors),
condition_c = ifelse(condition < 3, 3, condition),
renovation_date_c = ifelse(renovation_date == 0, 0, 1)) %>%
select(-c(num_bed, num_bath, num_floors, property_d, condition, renovation_date))
name(houses)
names(houses)
# num_bed
categoricalCoerc <- houses %>%
select(c(num_bed, num_bath, num_floors, condition, renovation_date)) %>%
mutate(num_bed_c = ifelse(num_bed < 3,
ifelse(num_bed > 4, 5, 2), num_bed),
num_bath_c = ifelse(num_bath < 1.5,
ifelse(num_bath > 3.5, 3.75, 1.25), num_bath),
num_floors_c = ifelse(num_floors > 2, 2.5, num_floors),
condition_c = ifelse(condition < 3, 3, condition),
renovation_date_c = ifelse(renovation_date == 0, 0, 1)) %>%
select(-c(num_bed, num_bath, num_floors, condition, renovation_date))
summary(categoricalCoerc)
categoricalCoerc <- houses %>%
select(c(num_bed, num_bath, num_floors, condition, renovation_date)) %>%
mutate(num_bed_c = ifelse(num_bed < 3,
2, ifelse(num_bed > 4, 5, num_bed)),
num_bath_c = ifelse(num_bath < 1.5,
1.25, ifelse(num_bath > 3.5, 3.75, num_bath)),
num_floors_c = ifelse(num_floors > 2, 2.5, num_floors),
condition_c = ifelse(condition < 3, 3, condition),
renovation_date_c = ifelse(renovation_date == 0, 0, 1)) %>%
select(-c(num_bed, num_bath, num_floors, condition, renovation_date))
summary(categoricalCoerc)
id <- 1:nrow(houses)
houses_id <- cbind(id, houses)
View(houses_id)
## Features Geográficas
geo <- houses %>% select(id, price, latitude, longitude)
## Features Geográficas
geo <- houses_id %>% select(id, price, latitude, longitude)
ggplot(categoricalCoerc, aes(factor(renovation_date_c), houses$price)) +
geom_boxplot()
ggplot(categoricalCoerc, aes(factor(renovation_date_c), houses$price)) +
geom_bar()
var(categoricalCoerc$renovation_date_c)
ggplot(categoricalCoerc, aes(factor(renovation_date_c))) +
geom_bar()
summary(aes(factor(categoricalCoerc$renovation_date_c)))
summary(factor(categoricalCoerc$renovation_date_c))
# is_waterfront - baixíssima variância e impacto muito razoável no preço
summary(factor(is_waterfront))
# is_waterfront - baixíssima variância e impacto muito razoável no preço
summary(factor(houses$is_waterfront))
summary(factor(categoricalCoerc$renovation_date_c))
View(houses)
# Sumarização das novas Features
houses_nf <- cbind(houses, latLon_cluster, categoricalCoerc)
View(houses_nf)
# Sumarização das novas Features
houses_nf <- cbind(houses, latLon_cluster, categoricalCoerc) %>%
select(-c(latitude, longitude, num_bed, num_bath, num_floors, condition, renovation_date, is_waterfront))
View(houses_nf)
names(houses_nf)
houses_fact <- houses_nf %>%
mutate_at(
.vars = vars('zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'),
.funs = funs(as.factor(.))
)
glimpse(houses_fact)
# Sumarizando as colunas
summarizeColumns(houses_fact) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# One hot encoding
houses_ohe <- createDummyFeatures(
houses_fact, target = "price",
cols = c(
'zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'
)
)
# Normalização
houses_prepoc <- normalizeFeatures(houses_ohe, target = "price")
View(houses_prepoc)
summarizeColumns(houses_prepoc) %>%
knitr::kable(digits = 2)
# factorização
houses_fact <- houses_nf %>%
mutate_at(
.vars = vars('zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'),
.funs = funs(as.factor(.))
)
# Sumarizando as colunas
summarizeColumns(houses_fact) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Normalização
houses_norm <- normalizeFeatures(houses_fact, target = "price")
# One hot encoding
houses_ohe <- createDummyFeatures(
houses_norm, target = "price",
cols = c(
'zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'
)
View(houses_ohe)
View(houses_ohe)
houses_fact <- houses_nf %>%
mutate_at(
.vars = vars('zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'),
.funs = funs(as.factor(.))
)
# Normalização
houses_norm <- normalizeFeatures(houses_fact, target = "price")
View(houses_norm)
# One hot encoding
houses_ohe <- createDummyFeatures(
houses_norm, target = "price",
cols = c(
'zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'
)
)
View(houses_ohe)
houses_prepoc = list()
# Criação do Task para Regressão (objeto usado nas operações do pacote)
fe_task <- makeRegrTask(id = 'Feature Engineering', data = houses_ohe, target = 'price')
# Separação do hold out usando os mesmo índices usados no baseline
fe_train <- subsetTask(fe_task, holdout$train.inds[[1]])
fe_test <- subsetTask(fe_task, holdout$test.inds[[1]])
X = getTaskData(base_test)[getTaskFeatureNames(base_test)]
base_predictor = Predictor$new(XGBoost, data = X, y = getTaskData(base_test)['price'])
base_imp = FeatureImp$new(base_predictor, loss = "rmse")
plot(base_imp)
# Treinamento
base_XGBoost <- train(xgb_tuned_base, base_train)
X = getTaskData(base_test)[getTaskFeatureNames(base_test)]
base_predictor = Predictor$new(XGBoost, data = X, y = getTaskData(base_test)['price'])
base_imp = FeatureImp$new(base_predictor, loss = "rmse")
plot(base_imp)
library(iml)
# usando Predictor$new() para criar um plot das features mais importantes
X = getTaskData(base_test)[getTaskFeatureNames(base_test)]
base_predictor = Predictor$new(XGBoost, data = X, y = getTaskData(base_test)['price'])
base_predictor = Predictor$new(base_XGBoost, data = X, y = getTaskData(base_test)['price'])
base_imp = FeatureImp$new(base_predictor, loss = "rmse")
plot(base_imp)
# tunador dos hiper parâmetros fe
tuned_params_base <- tuneParams(
learner = xgb_model,
task = fe_train,
resampling = resample_desc,
measures = rmse,
par.set = xgb_params,
control = control
)
tuned_params_fe <- tuneParams(
learner = xgb_model,
task = fe_train,
resampling = resample_desc,
measures = rmse,
par.set = xgb_params,
control = control
)
# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_fe,fe_train,resample_desc,measures = list(rmse, rsq))
xgb_tuned_fe <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params_fe$x
)
# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_fe,fe_train,resample_desc,measures = list(rmse, rsq))
# Treinamento
fe_XGBoost <- train(xgb_tuned_fe, fe_train)
# predição nos dados de teste
fe_pred_lm <- predict(fe_lm, fe_test)
# lm FE
fe_lm <- train(regr.lm, fe_train)
# predição nos dados de teste
fe_pred_lm <- predict(fe_lm, fe_test)
performance(fe_pred_lm, measures = list(rmse, rsq))
fe_pred_xgb <- predict(fe_XGBoost, fe_test)
performance(base_pred_xgb, measures = list(rmse, rsq))
base_pred_xgb <- predict(base_XGBoost, base_test)
performance(base_pred_xgb, measures = list(rmse, rsq))
## Baseline
# Criação do Task para Regressão (objeto usado nas operações do pacote)
Baseline_task <- makeRegrTask(id = 'Baseline', data = houses, target = 'price')
# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', Baseline_task)
base_train <- subsetTask(Baseline_task, holdout$train.inds[[1]])
base_test <- subsetTask(Baseline_task, holdout$test.inds[[1]])
## Modelo linear básico
# modelo padrão linear
regr.lm <- makeLearner(id = 'base_lm', 'regr.lm')
# treinamento modelo
base_lm <- train(regr.lm, base_train)
## XGBoost
# Criação do modelo simples
xgb_model <- makeLearner("regr.xgboost")
# Grid de alguns parâmetros
xgb_params <- makeParamSet(
# The number of trees in the model (each one built sequentially)
makeIntegerParam("nrounds", lower = 100, upper = 500),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 3) # quantas iterações o modelo aleatório de seleção fará
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params_base <- tuneParams(
learner = xgb_model,
task = base_train,
resampling = resample_desc,
measures = rmse,       # R-Squared performance measure, this can be changed to one or many
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_base <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params_base$x
)
# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_base,base_train,resample_desc,measures = list(rmse, rsq))
# pela grande diferença entre as cvs, o modelo está com bastante overffiting
# Treinamento
base_XGBoost <- train(xgb_tuned_base, base_train)
## Performance - Baseline
# predição nos dados de teste
base_pred_lm <- predict(base_lm, base_test)
performance(base_pred_lm, measures = list(rmse, rsq))
base_pred_xgb <- predict(base_XGBoost, base_test)
performance(base_pred_xgb, measures = list(rmse, rsq))
# usando Predictor$new() para criar um plot das features mais importantes
X = getTaskData(base_test)[getTaskFeatureNames(base_test)]
base_predictor = Predictor$new(base_XGBoost, data = X, y = getTaskData(base_test)['price'])
base_imp = FeatureImp$new(base_predictor, loss = "rmse")
plot(base_imp)
## Feature engineering
# Criação do Task para Regressão (objeto usado nas operações do pacote) FE
fe_task <- makeRegrTask(id = 'Feature Engineering', data = houses_ohe, target = 'price')
# Separação do hold out FE usando os mesmo índices usados no baseline
fe_train <- subsetTask(fe_task, holdout$train.inds[[1]])
fe_test <- subsetTask(fe_task, holdout$test.inds[[1]])
# lm FE
fe_lm <- train(regr.lm, fe_train)
# tunador dos hiper parâmetros FE
tuned_params_fe <- tuneParams(
learner = xgb_model,
task = fe_train,
resampling = resample_desc,
measures = rmse,
par.set = xgb_params,
control = control
)
# Modelo tunado FE
xgb_tuned_fe <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params_fe$x
)
# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_fe,fe_train,resample_desc,measures = list(rmse, rsq))
# vcs bem mais similares, indicando uma generalização mais acurada
# Treinamento
fe_XGBoost <- train(xgb_tuned_fe, fe_train)
## Performance - FE
# predição nos dados de teste
fe_pred_lm <- predict(fe_lm, fe_test)
performance(fe_pred_lm, measures = list(rmse, rsq))
fe_pred_xgb <- predict(fe_XGBoost, fe_test)
performance(base_pred_xgb, measures = list(rmse, rsq))
fe_pred_xgb <- predict(fe_XGBoost, fe_test)
performance(fe_pred_xgb, measures = list(rmse, rsq))
# Sumarização das novas Features
houses_nf <- cbind(houses, latLon_cluster, categoricalCoerc) %>%
select(-c(latitude, longitude, num_bed, num_bath, num_floors, condition, renovation_date, is_waterfront, zip))
## One hot Encoding
# factorização
houses_fact <- houses_nf %>%
mutate_at(
.vars = vars('zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'),
.funs = funs(as.factor(.))
)
# Sumarizando as colunas
summarizeColumns(houses_fact) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Normalização
houses_norm <- normalizeFeatures(houses_fact, target = "price")
# One hot encoding
houses_ohe <- createDummyFeatures(
houses_norm, target = "price",
cols = c(
'zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'
)
)
# One hot encoding
houses_ohe <- createDummyFeatures(
houses_norm, target = "price",
cols = c(
'zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'
)
)
# Sumarização das novas Features
houses_nf <- cbind(houses, latLon_cluster, categoricalCoerc) %>%
select(-c(latitude, longitude, num_bed, num_bath, num_floors, condition, renovation_date, is_waterfront, zip))
View(houses_nf)
# factorização
houses_fact <- houses_nf %>%
mutate_at(
.vars = vars('latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'),
.funs = funs(as.factor(.))
)
houses_norm <- normalizeFeatures(houses_fact, target = "price")
# One hot encoding
houses_ohe <- createDummyFeatures(
houses_norm, target = "price",
cols = c(
'zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'
)
)
# One hot encoding
houses_ohe <- createDummyFeatures(
houses_norm, target = "price",
cols = c(
'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'
)
)
## Feature engineering
# Criação do Task para Regressão (objeto usado nas operações do pacote) FE
fe_task <- makeRegrTask(id = 'Feature Engineering', data = houses_ohe, target = 'price')
# Separação do hold out FE usando os mesmo índices usados no baseline
fe_train <- subsetTask(fe_task, holdout$train.inds[[1]])
fe_test <- subsetTask(fe_task, holdout$test.inds[[1]])
# lm FE
fe_lm <- train(regr.lm, fe_train)
# tunador dos hiper parâmetros FE
tuned_params_fe <- tuneParams(
learner = xgb_model,
task = fe_train,
resampling = resample_desc,
measures = rmse,
par.set = xgb_params,
control = control
)
# Modelo tunado FE
xgb_tuned_fe <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params_fe$x
)
# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_fe,fe_train,resample_desc,measures = list(rmse, rsq))
# cvs bem mais similares, indicando uma generalização mais acurada
# Treinamento
fe_XGBoost <- train(xgb_tuned_fe, fe_train)
## Performance - FE
# predição nos dados de teste
fe_pred_lm <- predict(fe_lm, fe_test)
performance(fe_pred_lm, measures = list(rmse, rsq))
fe_pred_xgb <- predict(fe_XGBoost, fe_test)
performance(fe_pred_xgb, measures = list(rmse, rsq))
base_pred_xgb <- predict(base_XGBoost, base_test)
performance(base_pred_xgb, measures = list(rmse, rsq))
# predição nos dados de teste
base_pred_lm <- predict(base_lm, base_test)
performance(base_pred_lm, measures = list(rmse, rsq))
head(getTaskData(fe_test))
# Sumarização das novas Features
houses_nf <- cbind(houses, latLon_cluster, categoricalCoerc) %>%
select(-c(latitude, longitude, num_bed, num_bath, num_floors, condition, renovation_date, is_waterfront))
## One hot Encoding
# factorização
houses_fact <- houses_nf %>%
mutate_at(
.vars = vars('zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'),
.funs = funs(as.factor(.))
)
# Sumarizando as colunas
summarizeColumns(houses_fact) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Normalização
houses_norm <- normalizeFeatures(houses_fact, target = "price")
# One hot encoding
houses_ohe <- createDummyFeatures(
houses_norm, target = "price",
cols = c(
'zip', 'latLon_cluster','num_bed_c', 'num_bath_c', 'num_floors_c', 'condition_c',
'renovation_date_c'
)
)
# Criação do Task para Regressão (objeto usado nas operações do pacote) FE
fe_task <- makeRegrTask(id = 'Feature Engineering', data = houses_ohe, target = 'price')
# Separação do hold out FE usando os mesmo índices usados no baseline
fe_train <- subsetTask(fe_task, holdout$train.inds[[1]])
fe_test <- subsetTask(fe_task, holdout$test.inds[[1]])
# lm FE
fe_lm <- train(regr.lm, fe_train)
# tunador dos hiper parâmetros FE
tuned_params_fe <- tuneParams(
learner = xgb_model,
task = fe_train,
resampling = resample_desc,
measures = rmse,
par.set = xgb_params,
control = control
)
# Modelo tunado FE
xgb_tuned_fe <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params_fe$x
)
# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_fe,fe_train,resample_desc,measures = list(rmse, rsq))
# cvs bem mais similares, indicando uma generalização mais acurada
# Treinamento
fe_XGBoost <- train(xgb_tuned_fe, fe_train)
## Performance - FE
# predição nos dados de teste
fe_pred_lm <- predict(fe_lm, fe_test)
performance(fe_pred_lm, measures = list(rmse, rsq))
fe_pred_xgb <- predict(fe_XGBoost, fe_test)
performance(fe_pred_xgb, measures = list(rmse, rsq))
# Criação do Task para Regressão (objeto usado nas operações do pacote)
Baseline_task <- makeRegrTask(id = 'Baseline', data = houses, target = 'price')
# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', Baseline_task)
base_train <- subsetTask(Baseline_task, holdout$train.inds[[1]])
base_test <- subsetTask(Baseline_task, holdout$test.inds[[1]])
## Modelo linear básico
# modelo padrão linear
regr.lm <- makeLearner(id = 'base_lm', 'regr.lm')
# treinamento modelo
base_lm <- train(regr.lm, base_train)
## XGBoost
# Criação do modelo simples
xgb_model <- makeLearner("regr.xgboost")
# Grid de alguns parâmetros
xgb_params <- makeParamSet(
# The number of trees in the model (each one built sequentially)
makeIntegerParam("nrounds", lower = 100, upper = 500),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 3) # quantas iterações o modelo aleatório de seleção fará
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params_base <- tuneParams(
learner = xgb_model,
task = base_train,
resampling = resample_desc,
measures = rmse,       # R-Squared performance measure, this can be changed to one or many
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_base <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params_base$x
)
# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_base,base_train,resample_desc,measures = list(rmse, rsq))
# pela grande diferença entre as cvs, o modelo está com bastante overffiting
# Treinamento
base_XGBoost <- train(xgb_tuned_base, base_train)
## Performance - Baseline
# predição nos dados de teste
base_pred_lm <- predict(base_lm, base_test)
performance(base_pred_lm, measures = list(rmse, rsq))
base_pred_xgb <- predict(base_XGBoost, base_test)
performance(base_pred_xgb, measures = list(rmse, rsq))
# usando Predictor$new() para criar um plot das features mais importantes
X = getTaskData(base_test)[getTaskFeatureNames(base_test)]
base_predictor = Predictor$new(base_XGBoost, data = X, y = getTaskData(base_test)['price'])
pdp_area = Partial$new(base_predictor, feature = "size_house")
plot(pdp_area)
base_predictor = Predictor$new(base_XGBoost, data = X, y = getTaskData(base_test)['price'])
pdp_area = Partial$new(base_predictor, feature = "size_house")
plot(pdp_area)
